{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "57W9CBSAzUFs",
        "outputId": "08572855-bcdb-44c5-bfa9-583924dd523e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ8D6SptzYaf",
        "outputId": "69c705ac-9828-40b6-b591-bd50411665ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "XBLa4Rj6Fs2o"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "7OeCocg80wdz"
      },
      "outputs": [],
      "source": [
        "BASE = \"/content/drive/MyDrive/Colab Notebooks/A4-do-you-agree\"\n",
        "ARTIFACTS = BASE + \"/artifacts\"\n",
        "os.makedirs(ARTIFACTS, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASAM4tL91NcQ",
        "outputId": "7a02ac34-bb77-45b8-9ff6-1e3623dd9628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLGcZrwR05p0"
      },
      "source": [
        "# **TASK 1: Training BERT from Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVNUvOvE1T14"
      },
      "source": [
        "- **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-Nmoj4951Pqs",
        "outputId": "517bfd8b-3e96-49b9-cda5-399368a93679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples: 21224\n",
            "Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .\n"
          ]
        }
      ],
      "source": [
        "dt = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "texts = [t.strip() for t in dt[\"train\"][\"text\"] if len(t.split()) > 5]\n",
        "texts = texts[:100000] # 100k samples\n",
        "\n",
        "print(\"Samples:\", len(texts))\n",
        "print(texts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXLKbm8F8iXX"
      },
      "source": [
        "\n",
        "> **Dataset:** We use WikiText-2 (wikitext-2-raw-v1) from HuggingFace datasets. After filtering short lines (<5 tokens), the dataset contains 21,224 training samples.\n",
        "This subset is used to enable efficient training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaExltMu1pke"
      },
      "source": [
        "- **Build Vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdweKlFo1lp9",
        "outputId": "bfe1c2ba-ab02-4d39-9ad2-f3bdac4deab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 20005 PAD id: 0\n"
          ]
        }
      ],
      "source": [
        "special_tokens = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
        "\n",
        "words = \" \".join(texts).lower().split()\n",
        "vocab = Counter(words)\n",
        "\n",
        "vocab_size = 20000\n",
        "most_common = vocab.most_common(vocab_size)\n",
        "\n",
        "word2idx = {t:i for i,t in enumerate(special_tokens)}\n",
        "for w,_ in most_common:\n",
        "    if w not in word2idx:\n",
        "        word2idx[w] = len(word2idx)\n",
        "\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "PAD = word2idx[\"[PAD]\"]\n",
        "CLS = word2idx[\"[CLS]\"]\n",
        "SEP = word2idx[\"[SEP]\"]\n",
        "MASK = word2idx[\"[MASK]\"]\n",
        "UNK = word2idx[\"[UNK]\"]\n",
        "\n",
        "print(\"Vocab size:\", len(word2idx), \"PAD id:\", PAD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJz_6r-d10IB"
      },
      "source": [
        "- **Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "n84aZGTj1tSQ"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return [word2idx.get(w, UNK) for w in text.lower().split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmCEixFc131x"
      },
      "source": [
        "- **MLM Mask Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "u9dbxlr117Al"
      },
      "outputs": [],
      "source": [
        "def bert_mlm_mask(input_ids, mlm_prob=0.15):   # 80% : 10% : 10%\n",
        "\n",
        "    labels = [-100] * len(input_ids)\n",
        "    masked = input_ids.copy()\n",
        "\n",
        "    for i in range(len(input_ids)):\n",
        "        token_id = input_ids[i]\n",
        "        if token_id in (PAD, CLS, SEP):\n",
        "            continue\n",
        "\n",
        "        if random.random() < mlm_prob:\n",
        "            labels[i] = token_id\n",
        "            r = random.random()\n",
        "            if r < 0.8:\n",
        "                masked[i] = MASK\n",
        "            elif r < 0.9:\n",
        "                masked[i] = random.randint(len(special_tokens), len(word2idx)-1)\n",
        "            else:\n",
        "                masked[i] = token_id\n",
        "    return masked, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HANl2WXq1-EM"
      },
      "source": [
        "- **Make Sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "X34TU75Z2B9d"
      },
      "outputs": [],
      "source": [
        "max_len = 128\n",
        "\n",
        "def make_sample(text):\n",
        "    tokens = tokenize(text)[:max_len-2]\n",
        "    input_ids = [CLS] + tokens + [SEP]\n",
        "\n",
        "    # attention mask: 1=real token, 0=pad\n",
        "    attn_mask = [1] * len(input_ids)\n",
        "\n",
        "    # pad\n",
        "    pad_len = max_len - len(input_ids)\n",
        "    input_ids = input_ids + [PAD]*pad_len\n",
        "    attn_mask = attn_mask + [0]*pad_len\n",
        "\n",
        "    masked_ids, labels = bert_mlm_mask(input_ids)\n",
        "\n",
        "    return masked_ids, labels, attn_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvVYQMMm2LKQ"
      },
      "source": [
        "- **Mini BERT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "S3HvWGlB2OAj"
      },
      "outputs": [],
      "source": [
        "class MiniBERT(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden=256, max_len=64, n_layers=4, n_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden = hidden\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embed = nn.Embedding(vocab_size, hidden, padding_idx=PAD)\n",
        "        self.pos_embed = nn.Embedding(max_len, hidden)\n",
        "\n",
        "        self.ln = nn.LayerNorm(hidden)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model=hidden,nhead=n_heads,dropout=dropout,batch_first=True,activation=\"gelu\")\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.mlm_head = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask):\n",
        "\n",
        "        B, L = input_ids.shape\n",
        "        pos = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, L)\n",
        "\n",
        "        x = self.tok_embed(input_ids) + self.pos_embed(pos)\n",
        "        x = self.drop(self.ln(x))\n",
        "\n",
        "        src_key_padding_mask = (attn_mask == 0)\n",
        "\n",
        "        h = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        logits = self.mlm_head(h)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QmEeXABz2ftk"
      },
      "outputs": [],
      "source": [
        "hidden = 256\n",
        "model = MiniBERT(len(word2idx), hidden=hidden, max_len=max_len).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMjoFaco2z0r"
      },
      "source": [
        "- **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h4y-PUPK21aB",
        "outputId": "460453b6-161b-4d37-b371-40f5c9fd994d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1327it [01:22, 16.17it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: avg loss = 6.8058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1327it [01:19, 16.65it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: avg loss = 6.4891\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1327it [01:21, 16.37it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: avg loss = 6.3583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1327it [01:20, 16.54it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: avg loss = 6.2571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1327it [01:19, 16.64it/s]                          "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: avg loss = 6.1693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "epochs = 5\n",
        "train_texts = texts[:100000]\n",
        "\n",
        "def batch_iter(data, bs):\n",
        "    for i in range(0, len(data), bs):\n",
        "        yield data[i:i+bs]\n",
        "\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for batch_texts in tqdm(batch_iter(train_texts, batch_size), total=len(train_texts)//batch_size):\n",
        "        batch_inp, batch_lbl, batch_attn = [], [], []\n",
        "        for t in batch_texts:\n",
        "            inp, lbl, attn = make_sample(t)\n",
        "            batch_inp.append(inp)\n",
        "            batch_lbl.append(lbl)\n",
        "            batch_attn.append(attn)\n",
        "\n",
        "        input_ids = torch.tensor(batch_inp, dtype=torch.long).to(device)\n",
        "        labels = torch.tensor(batch_lbl, dtype=torch.long).to(device)\n",
        "        attn_mask = torch.tensor(batch_attn, dtype=torch.long).to(device)\n",
        "\n",
        "        logits = model(input_ids, attn_mask)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)),labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Epoch {ep+1}: avg loss = {total_loss/steps:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcVqByal3JIj"
      },
      "source": [
        "- **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOyxtfvS3H19",
        "outputId": "3e35ac6a-8fe9-44b4-bb84-be4aa988f62e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /content/drive/MyDrive/Colab Notebooks/A4-do-you-agree/artifacts/BERT_scratch.pt\n"
          ]
        }
      ],
      "source": [
        "SAVE_PATH = ARTIFACTS + \"/BERT_scratch.pt\"\n",
        "\n",
        "model_cpu = model.to(\"cpu\")\n",
        "torch.save({\n",
        "    \"model_state\": model_cpu.state_dict(),\n",
        "    \"word2idx\": word2idx,\n",
        "    \"config\": {\n",
        "        \"hidden\": hidden,\n",
        "        \"max_len\": max_len,\n",
        "        \"n_layers\": 4,\n",
        "        \"n_heads\": 4,\n",
        "        \"vocab_size\": len(word2idx),\n",
        "        \"special_tokens\": special_tokens,\n",
        "        \"dataset\": \"wikitext-2-raw-v1 (subset)\"}},SAVE_PATH)\n",
        "\n",
        "print(\"Saved:\",SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOwoTpWi83Gr"
      },
      "source": [
        "### **Training Result:**\n",
        "The MLM training loss decreased from 6.80 to 6.17 across 5 epochs, indicating successful learning of contextual language representations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHnczKkR9IdB"
      },
      "source": [
        "# **Task 2: Sentence Embedding with Sentence BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5haj16L83__",
        "outputId": "035d0d06-7bf1-4203-91d1-eb5481275bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n"
          ]
        }
      ],
      "source": [
        "BASE = \"/content/drive/MyDrive/Colab Notebooks/A4-do-you-agree\"\n",
        "ARTIFACTS = BASE + \"/artifacts\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)\n",
        "\n",
        "BERT_CKPT = ARTIFACTS + \"/BERT_scratch.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEw6wEls9zZG"
      },
      "source": [
        "- **Load BERT checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV1zJaOH94td",
        "outputId": "3e88e5bf-d9d9-4afe-9b34-f1b52c66eb54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocab: 20005 max_len: 128 hidden: 256\n"
          ]
        }
      ],
      "source": [
        "ckpt = torch.load(BERT_CKPT, map_location=\"cpu\")\n",
        "word2idx = ckpt[\"word2idx\"]\n",
        "config = ckpt[\"config\"]\n",
        "\n",
        "PAD = word2idx[\"[PAD]\"]\n",
        "CLS = word2idx[\"[CLS]\"]\n",
        "SEP = word2idx[\"[SEP]\"]\n",
        "UNK = word2idx[\"[UNK]\"]\n",
        "\n",
        "max_len = config[\"max_len\"]\n",
        "hidden = config[\"hidden\"]\n",
        "\n",
        "print(\"Loaded vocab:\", len(word2idx), \"max_len:\", max_len, \"hidden:\", hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "V6VfUFe3985b"
      },
      "outputs": [],
      "source": [
        "class MiniBERT_Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden=256, max_len=128, n_layers=4, n_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden = hidden\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.tok_embed = nn.Embedding(vocab_size, hidden, padding_idx=PAD)\n",
        "        self.pos_embed = nn.Embedding(max_len, hidden)\n",
        "        self.ln = nn.LayerNorm(hidden)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model=hidden,nhead=n_heads,dropout=dropout,batch_first=True,activation=\"gelu\")\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask):\n",
        "        B, L = input_ids.shape\n",
        "        pos = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, L)\n",
        "\n",
        "        x = self.tok_embed(input_ids) + self.pos_embed(pos)\n",
        "        x = self.drop(self.ln(x))\n",
        "\n",
        "        src_key_padding_mask = (attn_mask == 0)\n",
        "        h = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwY5mEZD-MfF"
      },
      "source": [
        "- **Load encoder weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV2Exych-IzJ",
        "outputId": "dc863b68-3a13-4099-8d0b-911b5c656119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "missing: []\n",
            "unexpected: ['mlm_head.weight', 'mlm_head.bias']\n"
          ]
        }
      ],
      "source": [
        "encoder = MiniBERT_Encoder(vocab_size=len(word2idx),hidden=config[\"hidden\"],max_len=config[\"max_len\"],\n",
        "    n_layers=config.get(\"n_layers\", 4),n_heads=config.get(\"n_heads\", 4)).to(device)\n",
        "\n",
        "missing, unexpected = encoder.load_state_dict(ckpt[\"model_state\"], strict=False)\n",
        "\n",
        "print(\"missing:\", missing)\n",
        "print(\"unexpected:\", unexpected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkNNfbKF-aJR"
      },
      "source": [
        "- **Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "h6mJ3Hu5-d20"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return [word2idx.get(w, UNK) for w in text.lower().split()]\n",
        "\n",
        "def encode_inputs(text):\n",
        "    tokens = tokenize(text)[:max_len-2]\n",
        "    input_ids = [CLS] + tokens + [SEP]\n",
        "    attn_mask = [1]*len(input_ids)\n",
        "\n",
        "    pad_len = max_len - len(input_ids)\n",
        "    input_ids += [PAD]*pad_len\n",
        "    attn_mask += [0]*pad_len\n",
        "\n",
        "    return input_ids, attn_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0WPo76H-wDo"
      },
      "source": [
        "- **Mean Pooling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "G5nC22RO-zjE"
      },
      "outputs": [],
      "source": [
        "def mean_pool(token_embeds, attn_mask):\n",
        "    mask = attn_mask.unsqueeze(-1).float()\n",
        "    summed = (token_embeds * mask).sum(dim=1)\n",
        "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
        "    return summed / counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C3vY8Mi_NFr"
      },
      "source": [
        "- **SBERT & classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "KsmeRYIE_OdH"
      },
      "outputs": [],
      "source": [
        "class SBERTSoftmax(nn.Module):\n",
        "    def __init__(self, encoder, hidden):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Linear(hidden*3, 3)\n",
        "\n",
        "    def forward(self, ids_a, mask_a, ids_b, mask_b):\n",
        "        h_a = self.encoder(ids_a, mask_a)\n",
        "        h_b = self.encoder(ids_b, mask_b)\n",
        "\n",
        "        u = mean_pool(h_a, mask_a)\n",
        "        v = mean_pool(h_b, mask_b)\n",
        "\n",
        "        feats = torch.cat([u, v, torch.abs(u - v)], dim=-1)\n",
        "        return self.classifier(feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pZs6RRA3BoZe"
      },
      "outputs": [],
      "source": [
        "hidden = config[\"hidden\"]\n",
        "sbert = SBERTSoftmax(encoder, hidden).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0uMbFEL_YqD"
      },
      "source": [
        "- **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2XszUjVOBqtd"
      },
      "outputs": [],
      "source": [
        "snli = load_dataset(\"snli\")\n",
        "\n",
        "train_data = snli[\"train\"].filter(lambda x: x[\"label\"] != -1)\n",
        "val_data   = snli[\"validation\"].filter(lambda x: x[\"label\"] != -1)\n",
        "train_data = train_data.select(range(min(50000, len(train_data))))\n",
        "val_data   = val_data.select(range(min(10000, len(val_data))))\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(sbert.parameters(), lr=2e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "FZ0wkDbuDLb5"
      },
      "outputs": [],
      "source": [
        "def batch_encode_inputs(text_list):\n",
        "    B = len(text_list)\n",
        "    ids = torch.full((B, max_len), PAD, dtype=torch.long)\n",
        "    mask = torch.zeros((B, max_len), dtype=torch.long)\n",
        "\n",
        "    for i, text in enumerate(text_list):\n",
        "        toks = [word2idx.get(w, UNK) for w in text.lower().split()][:max_len-2]\n",
        "        seq = [CLS] + toks + [SEP]\n",
        "        L = len(seq)\n",
        "        ids[i, :L] = torch.tensor(seq, dtype=torch.long)\n",
        "        mask[i, :L] = 1\n",
        "\n",
        "    return ids, mask\n",
        "\n",
        "def iterate_minibatches(ds, bs, shuffle=True):\n",
        "    idxs = list(range(len(ds)))\n",
        "    if shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for i in range(0, len(ds), bs):\n",
        "        yield [ds[j] for j in idxs[i:i+bs]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXML6b1xDOeu",
        "outputId": "70606a73-43d5-4d1c-9041-e88e703b7bac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1563it [03:12,  8.13it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: avg loss = 0.9353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1563it [03:12,  8.14it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: avg loss = 0.8820\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1563it [03:22,  7.73it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: avg loss = 0.8432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1563it [03:12,  8.11it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: avg loss = 0.8078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1563it [03:10,  8.21it/s]                          "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: avg loss = 0.7724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "for ep in range(epochs):\n",
        "    sbert.train()\n",
        "    total_loss, steps = 0.0, 0\n",
        "\n",
        "    for batch in tqdm(iterate_minibatches(train_data, batch_size), total=len(train_data)//batch_size):\n",
        "        prem = [b[\"premise\"] for b in batch]\n",
        "        hypo = [b[\"hypothesis\"] for b in batch]\n",
        "        y = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long).to(device)\n",
        "\n",
        "        ids_a, mask_a = batch_encode_inputs(prem)\n",
        "        ids_b, mask_b = batch_encode_inputs(hypo)\n",
        "\n",
        "        ids_a, mask_a = ids_a.to(device), mask_a.to(device)\n",
        "        ids_b, mask_b = ids_b.to(device), mask_b.to(device)\n",
        "\n",
        "        logits = sbert(ids_a, mask_a, ids_b, mask_b)\n",
        "        loss = loss_fn(logits, y)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(sbert.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Epoch {ep+1}: avg loss = {total_loss/steps:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aplEFSlhIyjU"
      },
      "source": [
        "# **Task 3: Evaluation and Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HChuDdVODcju"
      },
      "source": [
        "- **Accuracy & Cosine similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qtlsrZDFMvp",
        "outputId": "1fc9e21d-84ba-4d12-e7e4-c660e99142ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/63 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n",
            "100%|██████████| 63/63 [00:00<00:00, 63.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val accuracy (2k subset): 0.6165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def eval_accuracy(sbert, val_data, batch_size=32, n_samples=2000, device=\"cuda\"):\n",
        "    sbert.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_small = val_data.select(range(min(n_samples, len(val_data))))\n",
        "\n",
        "        for batch in tqdm(iterate_minibatches(val_small, batch_size, shuffle=False),\n",
        "                          total=math.ceil(len(val_small)/batch_size)):\n",
        "            prem = [b[\"premise\"] for b in batch]\n",
        "            hypo = [b[\"hypothesis\"] for b in batch]\n",
        "            y = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long).to(device)\n",
        "\n",
        "            ids_a, mask_a = batch_encode_inputs(prem)\n",
        "            ids_b, mask_b = batch_encode_inputs(hypo)\n",
        "\n",
        "            ids_a, mask_a = ids_a.to(device), mask_a.to(device)\n",
        "            ids_b, mask_b = ids_b.to(device), mask_b.to(device)\n",
        "\n",
        "            logits = sbert(ids_a, mask_a, ids_b, mask_b)\n",
        "            pred = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "val_acc = eval_accuracy(sbert, val_data, batch_size=32, n_samples=2000, device=device)\n",
        "print(\"Val accuracy (2k subset):\", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br4u5KQIDgaa",
        "outputId": "7278a91a-ce58-4ea5-9bcf-e391ceda8eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity: 0.4799080193042755\n"
          ]
        }
      ],
      "source": [
        "sbert.eval()\n",
        "with torch.no_grad():\n",
        "    sentA = \"A man is playing guitar.\"\n",
        "    sentB = \"A person is performing music.\"\n",
        "\n",
        "    ids_a, mask_a = batch_encode_inputs([sentA])\n",
        "    ids_b, mask_b = batch_encode_inputs([sentB])\n",
        "\n",
        "    ids_a, mask_a = ids_a.to(device), mask_a.to(device)\n",
        "    ids_b, mask_b = ids_b.to(device), mask_b.to(device)\n",
        "\n",
        "    h_a = sbert.encoder(ids_a, mask_a)\n",
        "    h_b = sbert.encoder(ids_b, mask_b)\n",
        "\n",
        "    u = mean_pool(h_a, mask_a)\n",
        "    v = mean_pool(h_b, mask_b)\n",
        "\n",
        "    cos = F.cosine_similarity(u, v).item()\n",
        "    print(\"Cosine similarity:\", cos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp6J6SHEGKdB",
        "outputId": "2fef6412-d4dd-46c8-b842-5fe0047e1413"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [00:00<00:00, 69.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6165\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   entailment     0.6414    0.5988    0.6193       663\n",
            "      neutral     0.5989    0.6440    0.6206       677\n",
            "contradiction     0.6126    0.6061    0.6093       660\n",
            "\n",
            "     accuracy                         0.6165      2000\n",
            "    macro avg     0.6176    0.6163    0.6164      2000\n",
            " weighted avg     0.6175    0.6165    0.6165      2000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[397 145 121]\n",
            " [109 436 132]\n",
            " [113 147 400]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float64(0.6165)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def eval_report(sbert, val_data, batch_size=32, n_samples=2000, device=\"cuda\"):\n",
        "    sbert.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_small = val_data.select(range(min(n_samples, len(val_data))))\n",
        "\n",
        "        for batch in tqdm(iterate_minibatches(val_small, batch_size, shuffle=False),\n",
        "                          total=math.ceil(len(val_small)/batch_size)):\n",
        "            prem = [b[\"premise\"] for b in batch]\n",
        "            hypo = [b[\"hypothesis\"] for b in batch]\n",
        "            y = [b[\"label\"] for b in batch]\n",
        "\n",
        "            ids_a, mask_a = batch_encode_inputs(prem)\n",
        "            ids_b, mask_b = batch_encode_inputs(hypo)\n",
        "\n",
        "            ids_a, mask_a = ids_a.to(device), mask_a.to(device)\n",
        "            ids_b, mask_b = ids_b.to(device), mask_b.to(device)\n",
        "\n",
        "            logits = sbert(ids_a, mask_a, ids_b, mask_b)\n",
        "            pred = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
        "\n",
        "            y_true.extend(y)\n",
        "            y_pred.extend(pred)\n",
        "\n",
        "    acc = (np.array(y_true) == np.array(y_pred)).mean()\n",
        "\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred,target_names=[\"entailment\", \"neutral\", \"contradiction\"],digits=4))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    return acc\n",
        "\n",
        "eval_report(sbert, val_data, batch_size=32, n_samples=2000, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx0PtxQgCGaW"
      },
      "source": [
        "- **Save model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bAcHyiLCEyN",
        "outputId": "80a8f742-9b92-4100-8c48-713b48cd80f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /content/drive/MyDrive/Colab Notebooks/A4-do-you-agree/artifacts/SBERT_nli.pt\n"
          ]
        }
      ],
      "source": [
        "SAVE_SBERT = ARTIFACTS + \"/SBERT_nli.pt\"\n",
        "\n",
        "sbert_cpu = sbert.to(\"cpu\")\n",
        "torch.save({\n",
        "    \"encoder_state\": sbert_cpu.encoder.state_dict(),\n",
        "    \"classifier_state\": sbert_cpu.classifier.state_dict(),\n",
        "    \"word2idx\": word2idx,\n",
        "    \"config\": config,\n",
        "    \"label2id\": {\"entailment\":0, \"neutral\":1, \"contradiction\":2}}, SAVE_SBERT)\n",
        "\n",
        "print(\"Saved:\", SAVE_SBERT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13tHfMRxI9tl"
      },
      "source": [
        "### **Discussion and Analysis**\n",
        "\n",
        "The experimental results demonstrate that the proposed SBERT model successfully learns meaningful sentence representations using a Siamese architecture and SoftmaxLoss objective.\n",
        "\n",
        "Key observations:\n",
        "\n",
        "- The model achieves balanced performance across all three NLI classes.\n",
        "- Mean pooling effectively aggregates token representations into sentence embeddings.\n",
        "- Training BERT from scratch still produces reasonable semantic understanding despite limited data and computational resources.\n",
        "- The cosine similarity experiment confirms that embeddings encode semantic similarity beyond classification labels.\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations**\n",
        "\n",
        "Several limitations were observed during implementation:\n",
        "\n",
        "- Training BERT from scratch requires substantial computational resources and longer training time.\n",
        "- The model was trained on a subset of SNLI rather than the full dataset due to GPU constraints.\n",
        "- Simple whitespace tokenization limits linguistic understanding compared to subword tokenizers (e.g., WordPiece).\n",
        "\n",
        "---\n",
        "\n",
        "### **Potential Improvements**\n",
        "\n",
        "Future improvements may include:\n",
        "\n",
        "- Training on larger datasets such as MNLI or full SNLI.\n",
        "- Using subword tokenization (WordPiece/BPE).\n",
        "- Increasing model depth or training epochs.\n",
        "- Applying contrastive learning objectives for stronger semantic embeddings.\n",
        "- Fine-tuning with pretrained transformer weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBKii8D1JmHI"
      },
      "source": [
        "# **Task 4: Text similarity - Web Application Development**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YqBBbknJqWu"
      },
      "source": [
        "A Dash-based web application was developed to demonstrate sentence similarity and Natural Language Inference (NLI).\n",
        "\n",
        "Features:\n",
        "\n",
        "- Input two sentences\n",
        "- Generate SBERT embeddings\n",
        "- Compute cosine similarity\n",
        "- Predict NLI label (entailment / neutral / contradiction)\n",
        "- Display probability distribution\n",
        "\n",
        "The application loads the trained SBERT model from Task 2\n",
        "and performs real-time inference.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
