# ğŸ§  SBERT NLI Demo (SNLI)

**Sentenceâ€‘BERT (from scratch) + Natural Language Inference + Dash Web Application**

This project implements a complete NLP pipeline:

1. Train **BERT from scratch** using Masked Language Modeling (MLM)
2. Fineâ€‘tune as **Sentenceâ€‘BERT (SBERT)** with a Siamese architecture
3. Perform **Natural Language Inference (NLI)**
4. Deploy an interactive **Dash web application**

---
## âœ… Model Architecture

### MiniBERT Encoder (Task 1)
- Token embedding + positional embedding
- Transformer encoder blocks
- Hidden size: **256**
- Max sequence length: **128**
- Vocabulary size: **~20k**
- Training objective: **Masked Language Modeling (MLM)**

### SBERT Siamese Network (Task 2)

Two sentences share the same encoder:

```
u = encoder(sentence A)
v = encoder(sentence B)

feature = [u, v, |u âˆ’ v|]
output = Softmax(Wáµ€ Â· feature)
```

Pooling method:
- Mean pooling over token embeddings

NLI Classes:
- entailment
- neutral
- contradiction

---

## ğŸ“ Project Structure

```
A4-DO-YOU-AGREE/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                # Dash web application
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ BERT_scratch.pt       # MLM pretrained encoder
â”‚   â””â”€â”€ SBERT_nli.pt          # SBERT trained model
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ st126130_notebook_A4.ipynb
â”‚
â”œâ”€â”€ sample.png                # App screenshot
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Installation

### 1. (Optional) Create virtual environment

```bash
python -m venv venv
```

Activate:

**macOS / Linux**
```bash
source venv/bin/activate
```

**Windows**
```bash
venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

Required packages:
- torch
- numpy
- dash
- dash-bootstrap-components
- datasets
- tqdm
- scikit-learn

---

## â–¶ï¸ Running the Web Application

From project root:

```bash
python app/app.py
```

You should see:

```
Dash is running on http://127.0.0.1:8050/
```

Open browser:

ğŸ‘‰ http://127.0.0.1:8050/

---

## ğŸ§ª How to Use the Web Interface

1. Enter **Sentence A** and **Sentence B**
2. Click **Run Prediction**
3. Outputs:
   - Predicted NLI label
   - Cosine similarity
   - Class probability bars

Example:

```
Sentence A: A man is playing guitar.
Sentence B: A person is performing music.
Prediction: neutral
Cosine similarity â‰ˆ 0.48
```

---
## ğŸš€ Demo

![Demo](sample.png)

---
## ğŸ§¾ Dataset

| Dataset | Purpose |
|---|---|
| WikiTextâ€‘2 | MLM pretraining |
| SNLI | NLI fineâ€‘tuning |

Datasets loaded via HuggingFace `datasets`.

---

## ğŸ“Š Evaluation (Task 3)

The notebook includes:

- Classification report
- Accuracy metrics
- Confusion matrix
- Analysis and discussion

Accuracy achieved:

**0.6165 (SNLI subset)**

---

## ğŸ’¾ Model Checkpoints

`artifacts/` contains:

- **BERT_scratch.pt**
  - Encoder pretrained via MLM
- **SBERT_nli.pt**
  - Siamese SBERT weights
  - Classifier head
  - Vocabulary + config

---

## âœ… Assignment Tasks Covered

- **Task 1:** Train BERT from scratch
- **Task 2:** Implement Sentenceâ€‘BERT
- **Task 3:** Evaluation & analysis
- **Task 4:** Dash deployment

---

## ğŸ‘¤ Author

- Student Name: Aphisit Jaemyaem
- Student ID: **st126130**
- Course: Natural Language Processing

---

## ğŸ“œ License

Educational use only.
